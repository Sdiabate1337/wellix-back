#!/usr/bin/env python3
"""
Test basique de l'architecture LLM service.
V√©rifie les interfaces, task router et GPT-4 service (sans vraie API).
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le projet au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

import structlog

# Configuration du logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)


async def test_llm_interfaces():
    """Test les interfaces et data classes."""
    print("=== Test Interfaces LLM ===")
    
    try:
        from app.services.llm.interfaces import (
            LLMProvider, LLMTaskType, LLMTask, LLMResult, 
            EnrichmentConfig, QualityMetrics, ConfidenceLevel
        )
        
        # Test cr√©ation config
        config = EnrichmentConfig(
            primary_provider=LLMProvider.GPT_4O,
            fallback_providers=[LLMProvider.CLAUDE_3_5_SONNET, LLMProvider.GEMINI_PRO],
            enable_consensus_validation=True,
            enable_semantic_cache=True
        )
        print(f"‚úì Config cr√©√©e: primary={config.primary_provider}")
        print(f"‚úì Fallbacks: {len(config.fallback_providers)}")
        
        # Test cr√©ation t√¢che
        task = LLMTask(
            task_type=LLMTaskType.NUTRITION_ANALYSIS,
            prompt="Analysez ce produit nutritionnellement",
            data={
                "product_name": "Yaourt grec nature",
                "ingredients": ["lait", "ferments lactiques"],
                "nutrition_facts": {"protein": 10, "calories": 60}
            },
            requires_validation=True
        )
        print(f"‚úì T√¢che cr√©√©e: {task.task_type}")
        
        # Test m√©triques qualit√©
        metrics = QualityMetrics(
            data_completeness=0.9,
            logical_consistency=0.8,
            source_citation=0.7,
            confidence_calibration=0.8,
            format_compliance=1.0
        )
        print(f"‚úì M√©triques: score global={metrics.overall_score:.2f}")
        print(f"‚úì Niveau confiance: {metrics.confidence_level}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur interfaces: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_task_router():
    """Test le routeur de t√¢ches intelligent."""
    print("\n=== Test Task Router ===")
    
    try:
        from app.services.llm.task_router import IntelligentTaskRouter
        from app.services.llm.interfaces import EnrichmentConfig, LLMTaskType, LLMTask, LLMProvider
        
        # Configuration
        config = EnrichmentConfig(
            primary_provider=LLMProvider.GPT_4O,
            fallback_providers=[LLMProvider.CLAUDE_3_5_SONNET, LLMProvider.GEMINI_PRO],
            enable_cost_optimization=True
        )
        
        # Cr√©ation router
        router = IntelligentTaskRouter(config)
        print(f"‚úì Router cr√©√© avec {len(router.TASK_SPECIALIZATIONS)} sp√©cialisations")
        
        # Test routage simple
        task = LLMTask(
            task_type=LLMTaskType.NUTRITION_ANALYSIS,
            prompt="Test prompt",
            data={"test": "data"}
        )
        
        optimal_provider = await router.route_task(task)
        print(f"‚úì Routage nutrition: {optimal_provider}")
        
        # Test planification d'analyse
        product_data = {
            "product_name": "C√©r√©ales enrichies",
            "ingredients": ["bl√©", "sucre", "vitamines"],
            "nutrition_facts": {"fiber": 5, "protein": 8, "calories": 350},
            "marketing_claims": ["Riche en fibres", "Source de prot√©ines"]
        }
        
        planned_tasks = await router.plan_analysis(product_data)
        print(f"‚úì Analyse planifi√©e: {len(planned_tasks)} t√¢ches")
        
        for task in planned_tasks:
            print(f"  - {task.task_type} (priorit√© {task.priority})")
        
        # Test optimisation co√ªt
        optimized_tasks = await router.optimize_for_cost(planned_tasks)
        print(f"‚úì Optimisation co√ªt: {len(optimized_tasks)} t√¢ches")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur task router: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_consensus_validator():
    """Test le validateur de consensus."""
    print("\n=== Test Consensus Validator ===")
    
    try:
        from app.services.llm.consensus_validator import ConsensusValidator
        from app.services.llm.interfaces import LLMResult, LLMTaskType, LLMProvider, QualityMetrics
        from datetime import datetime
        
        # Cr√©ation validator
        validator = ConsensusValidator(
            min_providers=2,
            consensus_threshold=0.8
        )
        print(f"‚úì Validator cr√©√© avec seuil {validator.consensus_threshold}")
        
        # Cr√©ation r√©sultats simul√©s
        result1 = LLMResult(
            analysis={
                "health_score": 8.5,
                "nutritional_quality": "excellent",
                "allergens_detected": ["milk"],
                "recommendations": ["Consommation quotidienne recommand√©e"]
            },
            raw_response="{}",
            quality_metrics=QualityMetrics(0.9, 0.8, 0.7, 0.8, 1.0),
            confidence_score=0.9,
            provider_used=LLMProvider.GPT_4O,
            task_type=LLMTaskType.NUTRITION_ANALYSIS,
            processing_time=2.5
        )
        
        result2 = LLMResult(
            analysis={
                "health_score": 8.2,
                "nutritional_quality": "excellent", 
                "allergens_detected": ["milk"],
                "recommendations": ["Excellent choix pour petit-d√©jeuner"]
            },
            raw_response="{}",
            quality_metrics=QualityMetrics(0.8, 0.9, 0.6, 0.7, 1.0),
            confidence_score=0.85,
            provider_used=LLMProvider.CLAUDE_3_5_SONNET,
            task_type=LLMTaskType.NUTRITION_ANALYSIS,
            processing_time=3.1
        )
        
        # Test calcul consensus
        consensus_score = await validator.calculate_consensus_score([result1, result2])
        print(f"‚úì Score consensus: {consensus_score:.2f}")
        
        # Test r√©solution conflits (r√©sultats similaires)
        resolved = await validator.resolve_conflicts([result1, result2])
        print(f"‚úì R√©solution consensus: score sant√© {resolved.analysis.get('health_score')}")
        
        # Test m√©triques consensus
        from app.services.llm.consensus_validator import ConsensusMetrics
        
        # Simulation directe des m√©triques
        validator_instance = ConsensusValidator()
        metrics = await validator_instance._calculate_consensus_metrics([result1, result2])
        print(f"‚úì Accord global: {metrics.agreement_score:.2f}")
        print(f"‚úì Coh√©rence factuelle: {metrics.factual_consistency:.2f}")
        print(f"‚úì Champs en conflit: {len(metrics.conflicting_fields)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur consensus validator: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_semantic_cache():
    """Test le cache s√©mantique."""
    print("\n=== Test Semantic Cache ===")
    
    try:
        from app.services.llm.semantic_cache import SemanticCache
        from app.services.llm.interfaces import LLMResult, LLMTaskType, LLMProvider, QualityMetrics
        import tempfile
        
        # Cr√©ation cache avec dossier temporaire
        with tempfile.TemporaryDirectory() as temp_dir:
            cache = SemanticCache(
                cache_dir=temp_dir,
                max_entries=100,
                similarity_threshold=0.95,
                enable_persistence=False  # D√©sactiv√© pour test
            )
            
            print(f"‚úì Cache cr√©√©: max {cache.max_entries} entr√©es")
            
            # Donn√©es test
            product_data1 = {
                "product_name": "Yaourt grec nature",
                "ingredients": ["lait", "ferments lactiques"],
                "nutrition_facts": {"protein": 10, "calories": 60}
            }
            
            product_data2 = {
                "product_name": "Yaourt grec naturel",  # Tr√®s similaire
                "ingredients": ["lait √©cr√©m√©", "ferments lactiques"],
                "nutrition_facts": {"protein": 9.5, "calories": 58}
            }
            
            # R√©sultat √† stocker
            result = LLMResult(
                analysis={
                    "health_score": 8.5,
                    "nutritional_quality": "excellent",
                    "recommendations": ["Excellent source de prot√©ines"]
                },
                raw_response="{}",
                quality_metrics=QualityMetrics(0.9, 0.8, 0.7, 0.8, 1.0),
                confidence_score=0.9,
                provider_used=LLMProvider.GPT_4O,
                task_type=LLMTaskType.NUTRITION_ANALYSIS,
                processing_time=2.5
            )
            
            # Test stockage
            await cache.store_analysis(product_data1, result)
            print("‚úì R√©sultat stock√© dans cache")
            
            # Test r√©cup√©ration exacte
            cached_exact = await cache.get_similar_analysis(product_data1, threshold=1.0)
            if cached_exact:
                print("‚úì R√©cup√©ration exacte r√©ussie")
                print(f"‚úì Cache hit: {cached_exact.cache_hit}")
                print(f"‚úì Similarit√©: {cached_exact.cache_similarity}")
            else:
                print("‚úó R√©cup√©ration exacte √©chou√©e")
            
            # Test r√©cup√©ration similaire
            cached_similar = await cache.get_similar_analysis(product_data2, threshold=0.8)
            if cached_similar:
                print("‚úì R√©cup√©ration similaire r√©ussie")
                print(f"‚úì Similarit√© d√©tect√©e: {cached_similar.cache_similarity:.2f}")
            else:
                print("‚úó Aucune similarit√© d√©tect√©e")
            
            # Test calcul similarit√©
            similarity = await cache.calculate_similarity(product_data1, product_data2)
            print(f"‚úì Similarit√© calcul√©e: {similarity:.2f}")
            
            # Statistiques cache
            stats = cache.get_cache_stats()
            print(f"‚úì Stats cache: {stats['total_entries']} entr√©es, hit rate: {stats['hit_rate']:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur semantic cache: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_gpt4_service_structure():
    """Test la structure du service GPT-4 (sans appel API)."""
    print("\n=== Test Structure GPT-4 Service ===")
    
    try:
        from app.services.llm.gpt4_service import GPT4Service
        from app.services.llm.interfaces import LLMProvider, LLMTaskType, LLMTask
        import os
        
        # Test sans vraie cl√© API (juste structure)
        os.environ["OPENAI_API_KEY"] = "test-key-for-structure-only"
        
        # Test cr√©ation service
        service = GPT4Service(LLMProvider.GPT_4O)
        print(f"‚úì Service cr√©√©: {service.provider_name}")
        print(f"‚úì T√¢ches support√©es: {len(service.supported_tasks)}")
        print(f"‚úì Co√ªt par token: ${service.cost_per_token:.8f}")
        
        # Test configuration mod√®le
        config = service.config
        print(f"‚úì Mod√®le: {config['model']}")
        print(f"‚úì Contexte: {config['context_window']} tokens")
        print(f"‚úì Max tokens: {config['max_tokens']}")
        
        # Test estimation co√ªt
        task = LLMTask(
            task_type=LLMTaskType.NUTRITION_ANALYSIS,
            prompt="Analysez ce produit: Yaourt grec avec miel et noix",
            data={"test": "data"},
            max_tokens=1000
        )
        
        estimated_cost = await service.estimate_cost(task)
        print(f"‚úì Co√ªt estim√©: ${estimated_cost:.4f}")
        
        # Test validation r√©ponse
        valid_json = '{"health_score": 8.5, "quality": "excellent"}'
        invalid_json = 'pas du json valide'
        
        is_valid = await service.validate_response(valid_json, "json")
        print(f"‚úì Validation JSON valide: {is_valid}")
        
        is_invalid = await service.validate_response(invalid_json, "json")
        print(f"‚úì Validation JSON invalide: {is_invalid}")
        
        # Test statistiques
        stats = service.get_usage_stats()
        print(f"‚úì Stats initiales: {stats['total_requests']} requ√™tes")
        
        # Nettoyage
        del os.environ["OPENAI_API_KEY"]
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur GPT-4 service: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Point d'entr√©e principal des tests."""
    print("üß† Tests Architecture LLM Service")
    print("=" * 50)
    
    tests = [
        ("Interfaces LLM", test_llm_interfaces),
        ("Task Router Intelligent", test_task_router),
        ("Consensus Validator", test_consensus_validator),
        ("Semantic Cache", test_semantic_cache),
        ("GPT-4 Service Structure", test_gpt4_service_structure)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\nüß™ {test_name}")
        print("-" * 30)
        
        try:
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå Test √©chou√©: {e}")
            results.append((test_name, False))
        
        await asyncio.sleep(0.5)  # Pause entre tests
    
    # R√©sum√©
    print("\n" + "=" * 50)
    print("üìä R√âSUM√â DES TESTS")
    print("=" * 50)
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS√â" if result else "‚ùå √âCHOU√â"
        print(f"{status}: {test_name}")
        if result:
            passed += 1
    
    print(f"\nüéØ R√©sultat: {passed}/{len(results)} tests pass√©s")
    
    if passed == len(results):
        print("üéâ TOUS LES TESTS SONT PASS√âS!")
        print("‚úÖ Architecture LLM OP√âRATIONNELLE")
    else:
        print("‚ö†Ô∏è  Certains tests ont √©chou√©")
    
    print("\nüìã ARCHITECTURE LLM COMPL√àTE:")
    print("‚úì Interfaces abstraites (ISP)")
    print("‚úì Task Router intelligent")
    print("‚úì Consensus Validator")
    print("‚úì Semantic Cache")
    print("‚úì GPT-4 Service Provider")
    print("üì¶ Pr√™t pour Manager + int√©gration workflow!")


if __name__ == "__main__":
    asyncio.run(main())